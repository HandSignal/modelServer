{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import json\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터셋 배치로 저장\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "def load_json_data(folder_path, batch_size=1000):\n",
    "    json_data = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.json'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                with open(file_path, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                    json_data.append(data)\n",
    "                    # Save the batch and clear the list to free memory\n",
    "                    if len(json_data) >= batch_size:\n",
    "                        yield json_data\n",
    "                        json_data = []\n",
    "    if json_data:\n",
    "        yield json_data\n",
    "base_path = \"/content/drive/MyDrive/Data/1.Training/라벨링데이터/REAL/WORD/\"\n",
    "folders = [os.path.join(base_path, f\"{i:02d}\") for i in range(1, 17)]\n",
    "\n",
    "batch_size = 1000  \n",
    "batch_idx = 0\n",
    "\n",
    "for folder_path in folders:\n",
    "    for batch in load_json_data(folder_path, batch_size=batch_size):\n",
    "        pickle_path = f'/content/drive/MyDrive/dataset/dataset_batch_{batch_idx}.pkl'\n",
    "        with open(pickle_path, 'wb') as f:\n",
    "            pickle.dump(batch, f)\n",
    "        batch_idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_paths, max_num_keypoints):\n",
    "        #self.data = data\n",
    "        self.max_num_keypoints = max_num_keypoints\n",
    "        self.embedding_model = SentenceTransformer(\"jhgan/ko-sroberta-multitask\")\n",
    "        self.data_paths = data_paths\n",
    "\n",
    "    def load_batch_data(self, batch_data_paths):\n",
    "         for batch_path in batch_data_paths:\n",
    "             with open(batch_path, 'rb') as f:\n",
    "                 batch_data = pickle.load(f)\n",
    "                 yield batch_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(1 for _ in self.load_batch_data(self.data_paths))\n",
    "\n",
    "    #데이터 패딩\n",
    "    def _pad_tensor(self, tensor, target_length, pad_value=0):\n",
    "        pad_size = target_length - tensor.size(0)\n",
    "        return F.pad(tensor, (0, 0, 0, pad_size), value=pad_value)\n",
    "    \n",
    "    #입력 데이터 생성(텐서로)\n",
    "    def __getitem__(self, idx):\n",
    "      for batch_data in self.load_batch_data(self.data_paths):\n",
    "          item = batch_data[idx]\n",
    "\n",
    "          pose_keypoints = item['pose_keypoint']\n",
    "          left_hand_keypoints = item['left_hand_keypoint']\n",
    "          right_hand_keypoints = item['right_hand_keypoint']\n",
    "          meaning = item['meaning']\n",
    "\n",
    "          pose_x = [kp['x'] for keypoints in pose_keypoints for kp in keypoints]\n",
    "          pose_y = [kp['y'] for keypoints in pose_keypoints for kp in keypoints]\n",
    "          pose_z = [kp['z'] for keypoints in pose_keypoints for kp in keypoints]\n",
    "          pose_v = [kp['visibility'] for keypoints in pose_keypoints for kp in keypoints]\n",
    "\n",
    "          left_hand_x = [kp['x'] for keypoints in left_hand_keypoints for kp in keypoints]\n",
    "          left_hand_y = [kp['y'] for keypoints in left_hand_keypoints for kp in keypoints]\n",
    "          left_hand_z = [kp['z'] for keypoints in left_hand_keypoints for kp in keypoints]\n",
    "\n",
    "          right_hand_x = [kp['x'] for keypoints in right_hand_keypoints for kp in keypoints]\n",
    "          right_hand_y = [kp['y'] for keypoints in right_hand_keypoints for kp in keypoints]\n",
    "          right_hand_z = [kp['z'] for keypoints in right_hand_keypoints for kp in keypoints]\n",
    "\n",
    "          pose_tensor = torch.tensor([pose_x, pose_y, pose_z, pose_v]).float().transpose(0, 1)\n",
    "          left_hand_tensor = torch.tensor([left_hand_x, left_hand_y, left_hand_z]).float().transpose(0, 1)\n",
    "          right_hand_tensor = torch.tensor([right_hand_x, right_hand_y, right_hand_z]).float().transpose(0, 1)\n",
    "\n",
    "          target_length = self.max_num_keypoints\n",
    "          pose_tensor = self._pad_tensor(pose_tensor, target_length)\n",
    "          left_hand_tensor = self._pad_tensor(left_hand_tensor, target_length)\n",
    "          right_hand_tensor = self._pad_tensor(right_hand_tensor, target_length)\n",
    "\n",
    "          names = [meaning_item['attributes'][0]['name'] for meaning_item in meaning]\n",
    "          meaning_tensors = [torch.tensor(self.embedding_model.encode(name)).float() for name in names]\n",
    "          combined_meaning_tensor = torch.stack(meaning_tensors, dim=0)\n",
    "          combined_meaning_tensor = self._pad_tensor(combined_meaning_tensor, target_length)\n",
    "          mask = (combined_meaning_tensor != 0).float()\n",
    "\n",
    "\n",
    "          return pose_tensor, left_hand_tensor, right_hand_tensor, combined_meaning_tensor,mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 구성\n",
    "#손, 포즈 네트워크를 거쳐 feature 생성 -> 합쳐서 fc레이어 거침\n",
    "class SignLanguageTranslationModel(nn.Module):\n",
    "    def __init__(self, pose_input_dim, hand_input_dim, hidden_dim, output_dim):\n",
    "        super(SignLanguageTranslationModel, self).__init__()\n",
    "        #pose 네트워크\n",
    "        self.pose_lstm = nn.LSTM(input_size=pose_input_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        #hand 네트워크\n",
    "        self.hand_lstm = nn.LSTM(input_size=hand_input_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        #fc레이어\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  \n",
    "\n",
    "    def forward(self, pose_inputs, hand_inputs):\n",
    "        pose_features, _ = self.pose_lstm(pose_inputs)\n",
    "        hand_features, _ = self.hand_lstm(hand_inputs)\n",
    "        #input 합침\n",
    "        combined_features = torch.cat((pose_features[:, -1], hand_features[:, -1]), dim=1)  # Combine final LSTM outputs\n",
    "        outputs = self.fc(combined_features)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HyperParameters\n",
    "MAX_NUM_KEYPOINTS = 5000\n",
    "BATCH_SIZE = 64\n",
    "POSE_INPUT_DIM = 4 \n",
    "HAND_INPUT_DIM = 3 \n",
    "MEANING_INPUT_DIM = 768\n",
    "HIDDEN_DIM = 512\n",
    "OUTPUT_DIM = 768\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(folder_path):\n",
    "    json_data = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for filename in files:\n",
    "            if filename.endswith('.json'):\n",
    "                file_path = os.path.join(root, filename)\n",
    "                with open(file_path, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                    json_data.append(data)\n",
    "    return json_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 배치 데이터 로드\n",
    "base_path = \"/content/drive/MyDrive/dataset/\"\n",
    "batch_data_paths = [f'{base_path}dataset_batch_{i}.pkl' for i in range(55)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터셋 및 데이터로더 생성\n",
    "custom_dataset = CustomDataset(batch_data_paths, max_num_keypoints=MAX_NUM_KEYPOINTS)\n",
    "dataloader = DataLoader(custom_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 학습\n",
    "model = SignLanguageTranslationModel(\n",
    "    pose_input_dim=POSE_INPUT_DIM,  # (x, y, z, visibility)\n",
    "    hand_input_dim=HAND_INPUT_DIM * 2,  #  (x, y, z)\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    output_dim=OUTPUT_DIM\n",
    ").to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "#손실함수 및 optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        pose_inputs, left_hand_inputs, right_hand_inputs, meaning_inputs, mask = [x.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")) for x in batch]\n",
    "\n",
    "        pose_inputs = pose_inputs.float()\n",
    "        left_hand_inputs = left_hand_inputs.float()\n",
    "        right_hand_inputs = right_hand_inputs.float()\n",
    "        meaning_inputs = meaning_inputs.float()\n",
    "        hand_inputs = torch.cat((left_hand_inputs, right_hand_inputs), dim=2)  # 양 손의 input 합쳐 모델 학습\n",
    "\n",
    "        outputs = model(pose_inputs, hand_inputs)\n",
    "\n",
    "        loss = criterion(outputs.unsqueeze(1), meaning_inputs.float())  \n",
    "        # 마스크 적용\n",
    "        loss = loss * mask  # 마스크를 곱하여 패딩 부분의 손실을 0\n",
    "        loss = loss.sum() / mask.sum()  # 평균 손실 계산 \n",
    "        total_loss += loss.item() \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {average_loss:4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델 저장\n",
    "torch.save(model, \"train_All_test_embedding.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"train_All_test_embedding_state.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class CustomDatasetforTest(Dataset):\n",
    "    def __init__(self, data_paths, max_num_keypoints):\n",
    "        self.max_num_keypoints = max_num_keypoints\n",
    "        self.embedding_model = SentenceTransformer(\"jhgan/ko-sroberta-multitask\")\n",
    "        self.data_paths = data_paths\n",
    "\n",
    "        # 모든 데이터를 한 번에 로드하고 저장\n",
    "        self.data = []\n",
    "        for batch_data in self.load_batch_data(self.data_paths):\n",
    "            self.data.extend(batch_data)\n",
    "\n",
    "    def load_batch_data(self, batch_data_paths):\n",
    "        batch_data = []\n",
    "        for directory in batch_data_paths:\n",
    "            for root, _, files in os.walk(directory):\n",
    "                for filename in files:\n",
    "                    file_path = os.path.join(root, filename)\n",
    "                    if file_path.endswith('.json'):\n",
    "                        with open(file_path, 'r') as f:\n",
    "                            try:\n",
    "                                data = json.load(f)\n",
    "                                batch_data.append(data)\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                print(f\"Error loading {file_path}: {e}\")\n",
    "                                continue\n",
    "        return json.dumps(batch_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _pad_tensor(self, tensor, target_length, pad_value=0):\n",
    "        pad_size = target_length - tensor.size(0)\n",
    "        return F.pad(tensor, (0, 0, 0, pad_size), value=pad_value)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        pose_keypoints = item['pose_keypoint']\n",
    "        left_hand_keypoints = item['left_hand_keypoint']\n",
    "        right_hand_keypoints = item['right_hand_keypoint']\n",
    "        meaning = item['meaning']\n",
    "\n",
    "        pose_x = [kp['x'] for keypoints in pose_keypoints for kp in keypoints]\n",
    "        pose_y = [kp['y'] for keypoints in pose_keypoints for kp in keypoints]\n",
    "        pose_z = [kp['z'] for keypoints in pose_keypoints for kp in keypoints]\n",
    "        pose_v = [kp['visibility'] for keypoints in pose_keypoints for kp in keypoints]\n",
    "\n",
    "        left_hand_x = [kp['x'] for keypoints in left_hand_keypoints for kp in keypoints]\n",
    "        left_hand_y = [kp['y'] for keypoints in left_hand_keypoints for kp in keypoints]\n",
    "        left_hand_z = [kp['z'] for keypoints in left_hand_keypoints for kp in keypoints]\n",
    "\n",
    "        right_hand_x = [kp['x'] for keypoints in right_hand_keypoints for kp in keypoints]\n",
    "        right_hand_y = [kp['y'] for keypoints in right_hand_keypoints for kp in keypoints]\n",
    "        right_hand_z = [kp['z'] for keypoints in right_hand_keypoints for kp in keypoints]\n",
    "\n",
    "        pose_tensor = torch.tensor([pose_x, pose_y, pose_z, pose_v]).float().transpose(0, 1)\n",
    "        left_hand_tensor = torch.tensor([left_hand_x, left_hand_y, left_hand_z]).float().transpose(0, 1)\n",
    "        right_hand_tensor = torch.tensor([right_hand_x, right_hand_y, right_hand_z]).float().transpose(0, 1)\n",
    "\n",
    "        target_length = self.max_num_keypoints\n",
    "        pose_tensor = self._pad_tensor(pose_tensor, target_length)\n",
    "        left_hand_tensor = self._pad_tensor(left_hand_tensor, target_length)\n",
    "        right_hand_tensor = self._pad_tensor(right_hand_tensor, target_length)\n",
    "\n",
    "        names = [meaning_item['attributes'][0]['name'] for meaning_item in meaning]\n",
    "        meaning_tensors = [torch.tensor(self.embedding_model.encode(name)).float() for name in names]\n",
    "        combined_meaning_tensor = torch.stack(meaning_tensors, dim=0)\n",
    "        combined_meaning_tensor = self._pad_tensor(combined_meaning_tensor, target_length)\n",
    "        mask = (combined_meaning_tensor != 0).float()\n",
    "\n",
    "\n",
    "        return pose_tensor, left_hand_tensor, right_hand_tensor, combined_meaning_tensor,mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"/content/train_All_test_embedding.pth\", map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 손실 초기화\n",
    "total_loss = 0\n",
    "\n",
    "# 테스트 데이터셋 로딩\n",
    "test_json_data_paths = [\"/content/drive/MyDrive/Data/2.Validation/라벨링데이터/REAL/WORD/17\", \"/content/drive/MyDrive/Data/2.Validation/라벨링데이터/REAL/WORD/18\"]\n",
    "\n",
    "# DataLoader 생성\n",
    "test_dataset = CustomDatasetforTest(test_json_data_paths, max_num_keypoints=MAX_NUM_KEYPOINTS)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 CPU로 이동\n",
    "model.to(torch.device('cpu'))\n",
    "\n",
    "# 평가 결과를 저장할 파일 경로\n",
    "output_file = '/content/drive/MyDrive/predictions_tensor.json'\n",
    "\n",
    "# 결과를 저장할 리스트 초기화\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "\n",
    "# 평가 루프\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        pose_inputs, left_hand_inputs, right_hand_inputs, meaning_inputs, mask = [x.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")) for x in batch]\n",
    "\n",
    "        pose_inputs = pose_inputs.float()\n",
    "        left_hand_inputs = left_hand_inputs.float()\n",
    "        right_hand_inputs = right_hand_inputs.float()\n",
    "        meaning_inputs = meaning_inputs.float()\n",
    "\n",
    "        # 왼손, 오른손 데이터를 결합\n",
    "        hand_inputs = torch.cat((left_hand_inputs, right_hand_inputs), dim=2)\n",
    "\n",
    "        # 모델 예측\n",
    "        outputs = model(pose_inputs, hand_inputs)\n",
    "\n",
    "        # 손실 계산 (마스크 적용)\n",
    "        loss = criterion(outputs.unsqueeze(1), meaning_inputs.float())\n",
    "        loss = loss * mask  \n",
    "        loss = loss.sum() / mask.sum()  # 평균 손실 계산\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 예측값과 정답 저장\n",
    "        all_predictions.append(outputs)\n",
    "        all_targets.append(meaning_inputs)\n",
    "\n",
    "average_loss = total_loss / len(test_dataloader)\n",
    "print(f'Test Loss: {average_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def calculate_accuracy_f1(predictions, targets):\n",
    "    # 리스트에서 텐서로 변환\n",
    "    predictions_tensor = torch.cat(predictions, dim=0)\n",
    "    targets_tensor = torch.cat(targets, dim=0)\n",
    "\n",
    "    # 텐서를 CPU로 이동하여 numpy 배열로 변환\n",
    "    predictions_np = predictions_tensor.cpu().numpy()\n",
    "    targets_np = targets_tensor.cpu().numpy()\n",
    "\n",
    "    # 이진 분류로 간주하여 임계값을 설정하여 일치 여부 계산\n",
    "    predictions_binary = (predictions_np >= 0.5).astype(int)\n",
    "    targets_binary = (targets_np >= 0.5).astype(int)\n",
    "\n",
    "    # 정확도 계산\n",
    "    accuracy = np.mean(predictions_binary == targets_binary)\n",
    "\n",
    "    # F1 점수 계산\n",
    "    true_positives = np.sum(predictions_binary * targets_binary)\n",
    "    false_positives = np.sum(predictions_binary * (1 - targets_binary))\n",
    "    false_negatives = np.sum((1 - predictions_binary) * targets_binary)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives + 1e-10)\n",
    "    recall = true_positives / (true_positives + false_negatives + 1e-10)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "\n",
    "    return accuracy, f1_score\n",
    "\n",
    "# 평가 결과 계산\n",
    "accuracy, f1_score = calculate_accuracy_f1(all_predictions, all_targets)\n",
    "\n",
    "# 결과 출력\n",
    "print(f'Test Accuracy: {accuracy:.4f}')\n",
    "print(f'Test F1 Score: {f1_score:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
